# -*- coding: utf-8 -*-
"""NLP Group

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1va-qv1S4e05_aJ4-Hoobyc-lA6qJxHtt

ðŸ”µ Click here to [see](https://docs.google.com/document/d/1T1fvpUPzJco9OIoVBjITN6WQnFZm5zWg7DBAeKR4Uac/edit#heading=h.2dmlq0mem11g) our shared document to see our group notes.

## 1. Data Collection

ðŸ”´ In order to import dataset, you can hit enter many times so it will be imported all data.

- I already created a automatic code here, so it will be automatically enter to get 100,000 rows data for us.

Before you import dataset, please go to excel file of yelp dataset, converting date column into mm-dd-yyyy.
"""

import pandas as pd

# Initialize an empty list to store data chunks
data_chunks = []

# Number of rows to read in each iteration
chunk_size = 10000

# Number of desired rows and columns
target_rows = 100000

# Loop until you have enough rows
while len(data_chunks) * chunk_size < target_rows:
    # Read a chunk of data from the CSV file
    chunk = pd.read_csv("Yelp_dataset.csv", nrows=chunk_size)

    # Append the chunk to the list
    data_chunks.append(chunk)

# Concatenate the data chunks into a single DataFrame
yelp_dataset = pd.concat(data_chunks, ignore_index=True)

# Ensure that the DataFrame has exactly 100,000 rows
yelp_dataset = yelp_dataset.head(target_rows)

yelp_dataset.info()

"""## 2. Exploratory Data Analysis ðŸŸ¡ Anvita

### Create a dataset summary
"""

yelp_dataset[['categories', 'review_count','name']]

"""### Bar chart for top 3 business categories"""

import matplotlib.pyplot as plt

# Group all the variables and count the number of businesses in each category
category_counts = yelp_dataset['categories'].str.split('; ', expand=True).stack().value_counts()

# Selected top 3 categories
top_categories = category_counts.head(3)

# Plotting
plt.figure(figsize=(10, 6))
ax = top_categories.plot(kind='bar', color='skyblue')
plt.title('Top 3 Business Categories of Yelp')
plt.xlabel('Category')
plt.ylabel('Number of Businesses')
plt.xticks(rotation=90)
plt.tight_layout()

# Add data labels
for i, count in enumerate(top_categories):
    ax.text(i, count, str(count), ha='center', va='bottom')

plt.show()

"""ðŸŸ¡ Anvita:

I believe we can do top 3 business categories for number of review counts would be making sense fo our project cause we are focusing on number of reviews. Let me know how you think. I attached a code and visualization chart below for your reference.
"""

import matplotlib.pyplot as plt

# Assuming yelp_dataset is your DataFrame
# Grouping by 'categories' and summing up the 'review_count' for each category
category_review_counts = yelp_dataset.groupby('categories')['review_count'].sum()

# Sorting categories by review count in descending order
category_review_counts = category_review_counts.sort_values(ascending=False)

# Selecting top 3 categories
top_categories = category_review_counts.head(3)

# Plotting
plt.figure(figsize=(14, 10))
bars = top_categories[::-1].plot(kind='barh', color='skyblue')  # Reverse the order for horizontal bar chart
plt.title('Top 3 Categories by Total Review Counts')
plt.xlabel('Total Number of Review')
plt.ylabel('Category')

# Add data labels
for bar in bars.patches:
    # Get the width of the bar
    width = bar.get_width()
    # Add text annotation on the bar with a slight left offset
    plt.text(width - 0.05 * width, bar.get_y() + bar.get_height() / 2, f'{width:.0f}', ha='right', va='center')

plt.show()

"""**Bars, Breakfast & Brunch, Restaurants, Barbeque, Cajun/Creole, Cafes, Cocktail Bars, Nightlife**: This category has the highest number of reviews, totaling approximately 8.93 million. The category includes a variety of dining and nightlife options, suggesting a high level of customer engagement and interest in these services.

**Live/Raw Food, Restaurants, Seafood, Beer Bar, Beer, Wine & Spirits, Bars, Food, Nightlife:** The second category has around 8.43 million reviews, which also indicates a strong consumer presence in the food and beverage sector, with a particular emphasis on seafood and establishments serving alcohol.

**Venues & Event Spaces, Performing Arts, Arts & Entertainment, Hotels & Travel, Food, Convenience Stores, American (New), Beauty & Spas, Restaurants, Museums, Event Planning & Services, Hotels, Cinema, Resorts, Day Spas:** The third category, with approximately 4.21 million reviews, is the most diverse. It covers a wide range of services from event spaces and hotels to arts, entertainment, and various types of food services.

### Rating Distribution across Yelp Reviews
"""

import seaborn as sns

# Plot the rating distribution using a boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x='stars_x', data= yelp_dataset, color='skyblue')
plt.title('Rating Distribution across Yelp Reviews')
plt.xlabel('Rating')
plt.ylabel('Number of Reviews')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming yelp_dataset is your DataFrame

# Plotting two charts side by side
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Adding a big title above the subplots
fig.suptitle('Distribution of Rating Stars Versus Number of Reviews', fontsize=16, fontweight='bold')

# 1: Plotting a bar plot to see distribution of stars and total number of reviews.
sns.countplot(x='stars_x', data=yelp_dataset, ax=axes[0], color='skyblue')
axes[0].set_xlabel('Stars')
axes[0].set_ylabel('Number of Reviews')

# 2: Plotting the rating distribution using a boxplot
sns.boxplot(x='stars_x', data=yelp_dataset, ax=axes[1], color='salmon')


# Hide x-axis label on the second subplot
axes[1].set_xlabel('')
axes[1].set_ylabel('')

# Hide y-axis labels on the second subplot
axes[1].set_yticklabels([])

plt.tight_layout()
plt.show()

"""**The majority of Yelp reviews tend to be positive, centering around 4 stars.
There's a high occurrence of moderate ratings (3-4 stars) compared to extreme ones.**

**The presence of outliers on the lower end in the box plot could indicate particularly bad experiences or potentially harsher critics, while outliers on the higher end could represent exceptionally good experiences or less critical reviewers.**

### Sentiment and Star Rating Distribution
"""

# Plotting star rating distribution

plt.figure(figsize=(8, 5))
sns.countplot(x='stars_x', data= yelp_dataset, palette='Blues_r')
plt.title('Star Rating Distribution')
plt.xlabel('Star Rating')
plt.ylabel('Number of Reviews')
plt.tight_layout()
plt.show()

yelp_dataset.loc[yelp_dataset['stars_x'] == 3, 'sentiment'] = 'neutral'
yelp_dataset.loc[yelp_dataset['stars_x'] < 3, 'sentiment'] = 'negative'
yelp_dataset.loc[yelp_dataset['stars_x'] > 3, 'sentiment'] = 'positive'

sentiment_df = yelp_dataset[['stars_x','sentiment', 'text']]
sentiment_df



from matplotlib import pyplot as plt
import seaborn as sns

# Assuming sentiment_df is your DataFrame
sentiment_counts = sentiment_df.groupby('sentiment').size()

# Plotting the horizontal bar plot
ax = sentiment_counts.plot(kind='barh', color=sns.palettes.mpl_palette('Blues'))

# Adding data labels to each bar
for i, count in enumerate(sentiment_counts):
    ax.text(count, i, str(count), ha='left', va='center')

# Removing top and right spines
plt.gca().spines[['top', 'right']].set_visible(False)

plt.show()

yelp_dataset.info()

"""### Histogramfor Best Restaurants based on average star rating"""

import pandas as pd
import matplotlib.pyplot as plt

# Calculate the average ratings for each restaurant
avg_ratings = yelp_dataset.groupby('name')['stars_x'].mean()

# Set up the figure for plotting
plt.figure(figsize=(10, 6))

# Plotting the histogram of average star ratings
n, bins, patches = plt.hist(avg_ratings, bins=10, color='skyblue', edgecolor='black')  # Collect bin information

# Adding titles and labels
plt.title('Distribution of Average Star Ratings Across All Restaurants')
plt.xlabel('Average Star Rating')
plt.ylabel('Number of Restaurants')

# Label each bin
for i in range(len(n)):
    plt.text(bins[i]+0.2, n[i] + 0.6, str(int(n[i])), fontsize=12, ha='center', va='bottom')

# Ensure everything fits without overlapping
plt.tight_layout()

# Display the plot
plt.show()

"""### Line chart for Year trend of count of reviews"""

import pandas as pd
import matplotlib.pyplot as plt


# Converting date strings to datetime objects
yelp_dataset['date'] = pd.to_datetime(yelp_dataset['date'])

# Extracting year from the datetime
yelp_dataset['year'] = yelp_dataset['date'].dt.year

# Grouping by year and summing review counts
yearly_review_sum = yelp_dataset.groupby('year')['review_count'].sum().reset_index()

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(yearly_review_sum['year'], yearly_review_sum['review_count'], marker='o')
plt.title('Total Reviews per Year')
plt.xlabel('Year')

plt.ylabel('Sum of Review Counts')
plt.grid(True)
plt.show()

"""The line graph shows a clear upward trend in the number of reviews from the year 2006 to around 2014, suggesting increasing usage of Yelp and greater customer engagement over these years.

### Line chart for Month trend of count of reviews
"""

import pandas as pd
import matplotlib.pyplot as plt

# Converting date strings to datetime objects
yelp_dataset['date'] = pd.to_datetime(yelp_dataset['date'])

# Extracting month from the datetime
yelp_dataset['month'] = yelp_dataset['date'].dt.month

# Grouping by month and summing review counts
month_review_sum = yelp_dataset.groupby('month')['review_count'].sum().reset_index()

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(month_review_sum['month'], month_review_sum['review_count'], marker='o')
plt.title('Total Reviews per Month')
plt.xlabel('Month')
plt.ylabel('Sum of Review Counts')
plt.grid(True)
plt.show()

"""After 2014, there appears to be a plateau and slight fluctuations in the number of reviews, which could indicate a stabilization in Yelp's user activity or possibly increased competition from other review platforms.

# 3. Text Mining ðŸŸ¡ Onkar


ðŸ”´ before you begin, please zoom with me, and read the exploratory data analysis first,

and check if you see texts including other languages rather than languages. If you spot any other language, I suggest we can remove those rows.

### Step 1: Preprocessing Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from nltk.corpus import stopwords
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk import sent_tokenize
from gensim.utils import simple_preprocess

restaurant_df = yelp_dataset[yelp_dataset['categories'].str.contains('restaurant', case=False)]
restaurant_df = restaurant_df.reset_index(drop=True)

restaurant_df.columns

restaurant_df

restaurant_df = restaurant_df[['sentiment','text']]

"""#### Retrieves all rows in the DataFrame where the sentiment is negative

**This is to understand about negative customer reviews.**
"""

sentiment_map = {'neutral': 0, 'positive': 1, 'negative': 2}
restaurant_df['sentiment'] = restaurant_df['sentiment'].map(sentiment_map)

restaurant_df[restaurant_df['sentiment']==2]

"""#### Clean and preprocess text data by removing non-alphabetic characters, converting text to lowercase, tokenizing, lemmatizing words, and removing stopwords."""

ps = PorterStemmer()

lemmatizer=WordNetLemmatizer()

corpus = []
for i in range(0, len(restaurant_df['text'])):
    review = re.sub('[^a-zA-Z]', ' ', restaurant_df['text'][i])
    review = review.lower()
    review = review.split()
    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)



corpus

"""**This is my revised code for the lemmatize.It takes me 40s to run.**

- This wordclous shows the the most common words from all negative reviews.

"""

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Assuming restaurant_df is your DataFrame and it has been defined somewhere above this code
# Precompile stopwords list and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Remove non-alphabet characters and convert to lowercase
    text = re.sub('[^a-zA-Z]', ' ', text).lower()
    # Split text into words (tokenization)
    words = text.split()
    # Remove stopwords and lemmatize
    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    # Join words back into a single string
    return ' '.join(cleaned_words)

# Apply the preprocessing function to each text entry in the DataFrame
restaurant_df['cleaned_text'] = restaurant_df['text'].apply(preprocess_text)

# Create a single string from the 'cleaned_text' for word cloud generation
corpus_text = ' '.join(restaurant_df['cleaned_text'])

# Generate the word cloud
wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(corpus_text)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off axis numbers and ticks
plt.show()

"""The most prominent words are "good," "great," and "back,", "first time", "come back" suggesting that customers have a positive view of the food or service and are willing to return. "Ice cream" also stands out, which may indicate a popular item or offering."""



# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2500,binary=True,ngram_range=(1,3))
X = cv.fit_transform(corpus).toarray()

y= restaurant_df['sentiment']

y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

from sklearn.naive_bayes import MultinomialNB
sentiment_detect_model = MultinomialNB().fit(X_train, y_train)

y_pred=sentiment_detect_model.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report
score=accuracy_score(y_test,y_pred)
print(score)

# Creating the TFIDF model
from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(max_features=2500,ngram_range=(1,3))
X = tv.fit_transform(corpus).toarray()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

from sklearn.naive_bayes import MultinomialNB
tfdif_detect_model = MultinomialNB().fit(X_train, y_train)

#prediction
y_pred=tfdif_detect_model.predict(X_test)

score=accuracy_score(y_test,y_pred)
print(score)

from sklearn.metrics import classification_report
print(classification_report(y_pred,y_test))

from nltk import sent_tokenize
from gensim.utils import simple_preprocess

corpus[0]

nltk.download('punkt')
words=[]
for sent in corpus:
    sent_token=sent_tokenize(sent)
    for sent in sent_token:
        words.append(simple_preprocess(sent))

import gensim.downloader as api

wv = api.load('word2vec-google-news-300')

import gensim
model=gensim.models.Word2Vec(words,window=5,min_count=2)

model.wv.index_to_key

model.corpus_count

model.epochs

model.wv.similar_by_word('quality')

def avg_word2vec(doc):
    # remove out-of-vocabulary words
    #sent = [word for word in doc if word in model.wv.index_to_key]
    #print(sent)

    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)
                #or [np.zeros(len(model.wv.index_to_key))], axis=0)

!pip install tqdm

from tqdm import tqdm
type(model.wv.index_to_key)

X=[]
for i in tqdm(range(len(words))):
    print(" ",i)
    X.append(avg_word2vec(words[i]))

X_new=np.array(X)

X_new[3]

X_new.shape

from wordcloud import WordCloud
wordcloud = WordCloud().generate(str(words))
plt.figure(figsize=(12,12))
plt.imshow(wordcloud.recolor(random_state=2017))
plt.title('Most Frequent Words')
plt.axis("off")
plt.show()





"""### Tokenization and Bag-of-word**

### Topic Modelling
"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

# Download the necessary NLTK models (if not already installed)
nltk.download('punkt')

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Set of English stopwords
stop_words = set(stopwords.words('english'))

# Initialize lemmatizer and stemmer
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Regular expression patterns for cleaning text
replace_patterns = [
    (r'&lt;', ''),                  # Remove HTML special entities (e.g., &lt;)
    (r'&gt;', ''),                  # Remove HTML special entities (e.g., &gt;)
    (r'&amp;', ''),                 # Remove HTML special entities (e.g., &amp;)
    (r'@(\w+)', ''),                # Remove @someone
    (r'http\S+', ''),               # Remove URLs
    (r'#(\w+)', ''),                # Remove hashtags
    (r'\d+', ''),                   # Remove numbers
    (r'\s{2,}', ' ')                # Replace multiple whitespaces
]

def clean_text(text):
    # Remove unwanted patterns from texts
    for (pattern, repl) in replace_patterns:
        text = re.sub(pattern, repl, text)
    return text

# Function to tokenize text using regular expressions, remove stopwords, and perform lemmatization
def process_text(text):
    text = clean_text(text.lower())  # Clean and convert to lowercase
    words = re.findall(r'\b\w+\b', text)  # Tokenize text into words
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word
    # words = [stemmer.stem(word) for word in words]  # Stem each word (alternative to lemmatization)
    return words

# Assuming yelp_dataset is a DataFrame with a column 'text'
# Tokenize the 'text' column using a list comprehension with the improved function
yelp_dataset['tokens'] = [process_text(text) for text in yelp_dataset['text']]

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# Flatten the list of lists of tokens into a single list
all_tokens = [token for sublist in yelp_dataset['tokens'] for token in sublist]

# Calculate the frequency of each word
word_freq = Counter(all_tokens)

# Keep only the top 10 most frequent words
top_words = dict(word_freq.most_common(10))

# Create a WordCloud object with these top words
wordcloud = WordCloud(background_color='white', width=800, height=600).generate_from_frequencies(top_words)

# Display the generated WordCloud
plt.figure(figsize=(12,12))
plt.imshow(wordcloud, interpolation='bilinear')  # 'bilinear' interpolation makes the displayed image appear smoother
plt.title('Top 20 Most Frequent Words')
plt.axis("off")
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud

# Flatten the list of tokens into a single list and prepare it for vectorization
word_list = [' '.join(tokens) for tokens in yelp_dataset['tokens']]

# Custom CountVectorizer with lemmatization
class LemmaCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        lemm = WordNetLemmatizer()
        analyzer = super(LemmaCountVectorizer, self).build_analyzer()
        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))

# Initialize and fit the vectorizer
tf_vectorizer = LemmaCountVectorizer(max_df=0.95, min_df=2, stop_words='english', decode_error='ignore')
tf = tf_vectorizer.fit_transform(word_list)

# LDA Model
lda = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online', learning_offset=50., random_state=0)
lda.fit(tf)

# Function to print the top words in each topic
def print_top_words(model, feature_names, n_top_words):
    for index, topic in enumerate(model.components_):
        message = "Topic #{}: ".format(index)
        message += " ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
        print("="*70)

# Display top words for each topic
n_top_words = 20
print("Topics in LDA model: ")
tf_feature_names = tf_vectorizer.get_feature_names_out()
print_top_words(lda, tf_feature_names, n_top_words)

"""#### Topic 1: Customer service"""

# Function to generate and show a word cloud for a specific topic
def show_word_cloud(model, feature_names, topic_idx, n_words=20):
    topic = model.components_[topic_idx]
    # Generate a dictionary of word frequencies for this topic
    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-n_words - 1:-1]}
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)
    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('Topic #{}'.format(topic_idx))
    plt.show()

# Example usage: Show word cloud for topic 0
show_word_cloud(lda, tf_vectorizer.get_feature_names_out(), topic_idx=0)

"""The topic seems to revolve around customer service experiences, emphasizing the time aspect ("minute," "day," "time") and the interaction process ("asked," "said," "told").
Larger words indicate that these are the most frequently occurring words in the documents that were analyzed for this topic.

#### Topic 2: Food Order experiences
"""

# Function to generate and show a word cloud for a specific topic
def show_word_cloud(model, feature_names, topic_idx, n_words=20):
    topic = model.components_[topic_idx]
    # Generate a dictionary of word frequencies for this topic
    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-n_words - 1:-1]}
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)
    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('Topic #{}'.format(topic_idx))
    plt.show()

# Example usage: Show word cloud for topic 0
show_word_cloud(lda, tf_vectorizer.get_feature_names_out(), topic_idx=1)

"""The word "food" is the most prominent, indicating that the main subject is food quality or variety.
Terms like "good," "burger," "chicken," "cheese," "fry," and "salad" point to specific food items that are commonly discussed in this context.
Words such as "order," "server," and "table" hint at the service aspect of dining experiences.

#### Topic 3: Location
"""

# Function to generate and show a word cloud for a specific topic
def show_word_cloud(model, feature_names, topic_idx, n_words=20):
    topic = model.components_[topic_idx]
    # Generate a dictionary of word frequencies for this topic
    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-n_words - 1:-1]}
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)
    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('Topic #{}'.format(topic_idx))
    plt.show()

# Example usage: Show word cloud for topic 0
show_word_cloud(lda, tf_vectorizer.get_feature_names_out(), topic_idx=2)

"""The dominant word "place" combined with "great," "like," and "love" points to positive reviews about certain locations.
"Room," "hotel," "bar," and "area" suggest that the reviews might be about various facilities or establishments like hotels or bars.
The presence of "staff" and "friendly" indicates that customer service is a notable part of the discussions.

#### Topic 4: Aspects of eateries that people enjoy.
"""

# Function to generate and show a word cloud for a specific topic
def show_word_cloud(model, feature_names, topic_idx, n_words=20):
    topic = model.components_[topic_idx]
    # Generate a dictionary of word frequencies for this topic
    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-n_words - 1:-1]}
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)
    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('Topic #{}'.format(topic_idx))
    plt.show()

# Example usage: Show word cloud for topic 0
show_word_cloud(lda, tf_vectorizer.get_feature_names_out(), topic_idx=3)

"""#### Topic 5: A positive dining experience with a possible focus on seafood"""

# Function to generate and show a word cloud for a specific topic
def show_word_cloud(model, feature_names, topic_idx, n_words=20):
    topic = model.components_[topic_idx]
    # Generate a dictionary of word frequencies for this topic
    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-n_words - 1:-1]}
    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)
    # Plot
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title('Topic #{}'.format(topic_idx))
    plt.show()

# Example usage: Show word cloud for topic 0
show_word_cloud(lda, tf_vectorizer.get_feature_names_out(), topic_idx=4)

""""Food," "service," and "restaurant" are the most prominent words, indicating these are the central themes in the reviews.

The abundance of positive adjectives like "great," "good," "delicious," "best," and "nice" suggests that the reviews are generally favorable regarding the food and the experience.

The repeated mention of "shrimp" and "crab" could signal a particular interest or emphasis on seafood within the dataset.

### Word2Vec

-  We want to see which words are most likely around our target common words, including "great", "place", "time", "good", "service".
"""

import re
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize
from gensim.models import Word2Vec
from nltk.corpus import stopwords

# Train a Word2Vec model
model = Word2Vec(sentences=yelp_dataset['tokens'], vector_size=100, window=5, min_count=2, workers=4, sg=1)

#window:5 the model will look at 5 words before and 5 words after it to understand its context and usage.

"""#### Words Next to Great"""

model.predict_output_word(['great'], topn=10)

import networkx as nx
import matplotlib.pyplot as plt

# Assuming you already have a trained Word2Vec model named 'model'
similar_words = model.predict_output_word(['great'], topn=10)

# Create a directed graph
G = nx.DiGraph()

# Add edges between 'service' and its similar words
for word, similarity in similar_words:
    G.add_edge('Great', word, weight=similarity)

# Draw the graph
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1800, font_size=12, edge_color='black', linewidths=1, arrows=True)

# Draw edge labels
edge_labels = {(n1, n2): f"{d['weight']:.4f}" for n1, n2, d in G.edges(data=True)}  # Formatting similarity values to four decimal places
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title("Words Next to 'Time'")
plt.show()

"""The model predicts that words like "atmosphere," "job," "selection," and "beer" are likely to appear in contexts where the word "great" is used. This suggests that people often use "great" to describe places with good ambience, choices, scenic views, and beverages.

#### Words Next to Place
"""

model.predict_output_word(['place'], topn=10)

import networkx as nx
import matplotlib.pyplot as plt

# Assuming you already have a trained Word2Vec model named 'model'
similar_words = model.predict_output_word(['place'], topn=10)

# Create a directed graph
G = nx.DiGraph()

# Add edges between 'service' and its similar words
for word, similarity in similar_words:
    G.add_edge('Place', word, weight=similarity)

# Draw the graph
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1800, font_size=12, edge_color='black', linewidths=1, arrows=True)

# Draw edge labels
edge_labels = {(n1, n2): f"{d['weight']:.4f}" for n1, n2, d in G.edges(data=True)}  # Formatting similarity values to four decimal places
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title("Words Next to 'Place'")
plt.show()

"""Predictions such as "love," "recommend," and "town" indicate that "place" is commonly associated with positive feelings or recommendations. Words like "cute" and "chill" suggest descriptions of a place's vibe, while "chinese" hints at specific types of places, like restaurants.

#### Words Next to Time
"""

model.predict_output_word(['time'], topn=10)

import networkx as nx
import matplotlib.pyplot as plt

# Assuming you already have a trained Word2Vec model named 'model'
similar_words = model.predict_output_word(['time'], topn=10)

# Create a directed graph
G = nx.DiGraph()

# Add edges between 'service' and its similar words
for word, similarity in similar_words:
    G.add_edge('Time', word, weight=similarity)

# Draw the graph
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1800, font_size=12, edge_color='black', linewidths=1, arrows=True)

# Draw edge labels
edge_labels = {(n1, n2): f"{d['weight']:.4f}" for n1, n2, d in G.edges(data=True)}  # Formatting similarity values to four decimal places
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title("Words Next to 'Time'")
plt.show()

"""The model's output with "time" centers around descriptors of frequency and order, such as "first," "last," "every," and "next." This indicates common discussions about experiences or events occurring at different times or occasions.

#### Words Next to Good
"""

model.predict_output_word(['good'], topn=10)

import networkx as nx
import matplotlib.pyplot as plt

# Assuming you already have a trained Word2Vec model named 'model'
similar_words = model.predict_output_word(['good'], topn=10)

# Create a directed graph
G = nx.DiGraph()

# Add edges between 'service' and its similar words
for word, similarity in similar_words:
    G.add_edge('Good', word, weight=similarity)

# Draw the graph
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1800, font_size=12, edge_color='black', linewidths=1, arrows=True)

# Draw edge labels
edge_labels = {(n1, n2): f"{d['weight']:.4f}" for n1, n2, d in G.edges(data=True)}  # Formatting similarity values to four decimal places
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title("Words Next to 'Good'")
plt.show()

"""Words related to food items like "burger," "sandwich," and "fry" along with qualities like "pretty" and "portion" suggest that "good" is often used to describe satisfying meals and their attributes. "Atmosphere" also appears here, indicating that the environment of dining places is often noted alongside the quality of food.

#### Words Next to Service
"""

model.predict_output_word(['service'], topn=10)

import networkx as nx
import matplotlib.pyplot as plt

# Assuming you already have a trained Word2Vec model named 'model'
similar_words = model.predict_output_word(['service'], topn=10)

# Create a directed graph
G = nx.DiGraph()

# Add edges between 'service' and its similar words
for word, similarity in similar_words:
    G.add_edge('service', word, weight=similarity)

# Draw the graph
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=1800, font_size=12, edge_color='black', linewidths=1, arrows=True)

# Draw edge labels
edge_labels = {(n1, n2): f"{d['weight']:.4f}" for n1, n2, d in G.edges(data=True)}  # Formatting similarity values to four decimal places
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title("Words Next to 'Service'")
plt.show()

"""This set of predictions features a mix of positive and negative descriptors, such as "excellent," "impeccable," "slow," and "poor." These are commonly used to describe the quality of service, indicating that reviews frequently focus on the speed and attentiveness of service, reflecting customer satisfaction or dissatisfaction.

### Bag-Of-Word N-grams

#### Bi-grams: Two-words together
"""

import nltk
from nltk.util import ngrams
from collections import Counter
import pandas as pd
import string


from nltk.util import ngrams
from collections import Counter

def get_ngrams(tokens, n):
    n_grams = ngrams(tokens, n)
    return [' '.join(grams) for grams in n_grams]

# Flatten the list of token lists and remove punctuation
flattened_tokens = [token for sublist in yelp_dataset['tokens'] for token in sublist if token not in string.punctuation]

bigrams = get_ngrams(flattened_tokens, 2  )
bigrams_count = Counter(bigrams)

bigram_freq = pd.DataFrame.from_dict(bigrams_count, orient='index', columns=['frequency'])
bigram_freq = bigram_freq.sort_values(by='frequency', ascending=False).head(10)
print(bigram_freq)

# Create a bar plot
plt.figure(figsize=(12, 8))
plt.bar(bigram_freq.index, bigram_freq['frequency'], color='green')
plt.xlabel('Bigrams', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Top 10 Most Frequent Bigrams', fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate the x-axis labels for better visibility
plt.tight_layout()  # Adjust subplots to give some padding between the plots and the edges of the plotting area
plt.show()

"""**Customer Loyalty and Satisfaction:**

Bigrams like "go back," "highly recommend," and "come back" suggest a strong customer intent to revisit and endorse the businesses, indicative of high satisfaction and potential customer loyalty.

"First time" may point to the importance of first impressions, which can be critical for repeat business.

#### Tri-grams
"""

import nltk
from nltk.util import ngrams
from collections import Counter
import pandas as pd
import string


from nltk.util import ngrams
from collections import Counter

def get_ngrams(tokens, n):
    n_grams = ngrams(tokens, n)
    return [' '.join(grams) for grams in n_grams]

# Flatten the list of token lists and remove punctuation
flattened_tokens = [token for sublist in yelp_dataset['tokens'] for token in sublist if token not in string.punctuation]

bigrams = get_ngrams(flattened_tokens, 3)
bigrams_count = Counter(bigrams)

bigram_freq = pd.DataFrame.from_dict(bigrams_count, orient='index', columns=['frequency'])
bigram_freq = bigram_freq.sort_values(by='frequency', ascending=False).head(10)
print(bigram_freq)

# Create a bar plot
plt.figure(figsize=(12, 8))
plt.bar(bigram_freq.index, bigram_freq['frequency'], color='green')
plt.xlabel('Bigrams', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Top 10 Most Frequent Bigrams', fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate the x-axis labels for better visibility
plt.tight_layout()  # Adjust subplots to give some padding between the plots and the edges of the plotting area
plt.show()

"""Viewpoint on Customer Advocacy and Intent:

The phrases "would highly recommend" and "would definitely recommend" suggest that there is a strong propensity among customers to advocate for the businesses they've visited, indicating high satisfaction levels.
Repeated variations of "go back," including "wait go back," "definitely go back," and "would go back," indicate a strong intent to revisit, signaling good customer retention potential.
Focus on Menu Items and Food Quality:

Specific menu items like "mac n cheese" and "gumbo ya ya" suggest these dishes stand out in the customer's dining experience and may be signature items that attract customers.
The mention of "sweet potato fry" indicates that even side dishes can significantly impact customer satisfaction and are often remembered and mentioned in reviews.

#### Four-grams
"""

import nltk
from nltk.util import ngrams
from collections import Counter
import pandas as pd
import string


from nltk.util import ngrams
from collections import Counter

def get_ngrams(tokens, n):
    n_grams = ngrams(tokens, n)
    return [' '.join(grams) for grams in n_grams]

# Flatten the list of token lists and remove punctuation
flattened_tokens = [token for sublist in yelp_dataset['tokens'] for token in sublist if token not in string.punctuation]

bigrams = get_ngrams(flattened_tokens, 4)
bigrams_count = Counter(bigrams)

bigram_freq = pd.DataFrame.from_dict(bigrams_count, orient='index', columns=['frequency'])
bigram_freq = bigram_freq.sort_values(by='frequency', ascending=False).head(10)
print(bigram_freq)

# Create a bar plot
plt.figure(figsize=(12, 8))
plt.bar(bigram_freq.index, bigram_freq['frequency'], color='lightblue')
plt.xlabel('Bigrams', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('Top 10 Most Frequent Fourgrams', fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate the x-axis labels for better visibility
plt.tight_layout()  # Adjust subplots to give some padding between the plots and the edges of the plotting area
plt.show()

"""**Customer Engagement and Intent to Return:**

Phrases such as "would definitely go back" and variations like "come back next time" suggest a clear intent by customers to revisit, reflecting satisfaction and potential loyalty.
The repetition of "come back" in different four-grams reinforces the significance of return visits as a metric for success.
Menu Items and Service Quality:

Specific menu items or attributes like "bacon wrapped shrimp" and "cookie dough egg roll" highlight unique or memorable dishes that stand out to customers.
The combination of "great food great service" suggests that customers value the overall dining experience, with equal emphasis on the quality of the food and the level of service received.

#### Compares the frequency of bigrams, trigrams, and fourgrams
"""

from nltk import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import pandas as pd

def get_ngrams(tokens, n):
    return [' '.join(gram) for gram in ngrams(tokens, n)]

# Assuming flattened_tokens is already defined as a flat list of all tokens excluding punctuation
bigrams = get_ngrams(flattened_tokens, 2)
trigrams = get_ngrams(flattened_tokens, 3)
fourgrams = get_ngrams(flattened_tokens, 4)

# Count frequencies
bigram_count = Counter(bigrams)
trigram_count = Counter(trigrams)
fourgram_count = Counter(fourgrams)

# Convert to DataFrames
bigram_freq = pd.DataFrame.from_dict(bigram_count, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False).head(10)
trigram_freq = pd.DataFrame.from_dict(trigram_count, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False).head(10)
fourgram_freq = pd.DataFrame.from_dict(fourgram_count, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False).head(10)

import plotly.express as px

# Combine the dataframes for a unified plot with identification for each gram type
bigram_freq['n-gram'] = 'Bigram'
trigram_freq['n-gram'] = 'Trigram'
fourgram_freq['n-gram'] = 'Fourgram'

combined_df = pd.concat([bigram_freq, trigram_freq, fourgram_freq])
combined_df['N-gram Words'] = combined_df.index  # Make the index a column for easier plotting

# Create a larger figure to enhance readability
fig = px.bar(combined_df, x='N-gram Words', y='frequency', color='n-gram',
             title='Comparison of Top N-grams', labels={'N-gram Words': 'N-grams', 'frequency': 'Frequency'},
             barmode='group', height=600, width=1200)  # Increased size

# Customize the layout for better visibility
fig.update_layout(xaxis_title='N-grams',
                  yaxis_title='Frequency',
                  xaxis={'categoryorder':'total descending'},
                  legend_title_text='N-Gram Type',
                  xaxis_tickangle=-90)  # Adjust the angle of the x-axis labels for better readability

# Show the plot
fig.show()

"""The bigrams, being more frequent than trigrams and fourgrams, could indicate that customers often use short, concise phrases to describe their experiences.

The prevalence of phrases such as "highly recommend" and "would definitely recommend" among the bigrams and trigrams signals strong endorsements that can be leveraged in marketing materials to build credibility.

The dominance of "first time" among the bigrams may highlight the importance of first impressions, indicating the need for businesses to focus on the initial customer experience.

# Bi-Directional RNN
https://medium.com/@jeewonkim1028/sentiment-analysis-in-keras-using-attention-mechanism-on-yelp-reviews-dataset-322bd7333b8b
"""

yelp_dataset.info()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming yelp_dataset is your DataFrame
# Plotting the distribution of star ratings
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed

# Adding a title
plt.title('Distribution of Rating Stars Versus Number of Reviews', fontsize=16, fontweight='bold')

# Creating a bar plot for the distribution of stars
sns.countplot(x='stars_x', data=yelp_dataset, color='skyblue')  # Ensure the column name 'stars_x' is correct

# Setting labels
plt.xlabel('Stars', fontsize=12)
plt.ylabel('Number of Reviews', fontsize=12)

# Displaying the plot
plt.show()

"""We've noticed that most people tend to give 4 to 5-star ratings, indicating a positive sentiment. Building on this observation, we've introduced a new sentiment feature with values of 0 and 1. Reviews with ratings above 3 are labeled as "positive" with a value of 1, while others are considered "negative" with a value of 0."""

import numpy as np

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize


yelp_dataset['sentiment'] = np.where(yelp_dataset['stars_x'] > 3, 1, 0)

import nltk
nltk.download('stopwords')

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import re

# Download the stopwords from NLTK
nltk.download('stopwords')
nltk.download('wordnet')  # This might also be necessary if you haven't downloaded it yet

# Now set the English stopwords
stop_words = set(stopwords.words('english'))

# Initialize lemmatizer and stemmer
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Regular expression patterns for cleaning text
replace_patterns = [
    (r'&lt;', ''),                  # Remove HTML special entities (e.g., &lt;)
    (r'&gt;', ''),                  # Remove HTML special entities (e.g., &gt;)
    (r'&amp;', ''),                 # Remove HTML special entities (e.g., &amp;)
    (r'@(\w+)', ''),                # Remove @someone
    (r'http\S+', ''),               # Remove URLs
    (r'#(\w+)', ''),                # Remove hashtags
    (r'\d+', ''),                   # Remove numbers
    (r'\s{2,}', ' ')                # Replace multiple whitespaces
]

def clean_text(text):
    # Remove unwanted patterns from texts
    for (pattern, repl) in replace_patterns:
        text = re.sub(pattern, repl, text)
    return text

# Function to tokenize text using regular expressions, remove stopwords, and perform lemmatization
def process_text(text):
    text = clean_text(text.lower())  # Clean and convert to lowercase
    words = re.findall(r'\b\w+\b', text)  # Tokenize text into words
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word
    # words = [stemmer.stem(word) for word in words]  # Stem each word (alternative to lemmatization)
    return words

# Assuming yelp_dataset is a DataFrame with a column 'text'
# Tokenize the 'text' column using a list comprehension with the improved function
yelp_dataset['cleaned_text'] = [process_text(text) for text in yelp_dataset['text']]

# Selecting only the necessary columns to display
comparison_df = yelp_dataset[['text', 'cleaned_text']]

comparison_df.head(3)

#Split dataset into train and test

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Assuming yelp_dataset contains the 'cleaned_text' and 'sentiment' columns from earlier processes
df_train, df_test = train_test_split(yelp_dataset, test_size=0.01)  # Splitting the dataset

MAX_FEATURES = 6000  # Maximum number of words to consider in the vocabulary
EMBED_SIZE = 128  # Size of the embedding vectors
tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(df_train['cleaned_text'])  # Fit tokenizer to cleaned texts

list_tokenized_train = tokenizer.texts_to_sequences(df_train['cleaned_text'])  # Convert texts to sequences of integers
MAX_LEN = 60  # Maximum length of sequences, padded where necessary
X_train = pad_sequences(list_tokenized_train, maxlen=MAX_LEN)  # Pad sequences to ensure uniform input size

y_train = df_train['sentiment']  # Sentiment labels

X_train

y_train

"""Define Attention"""

import pandas as pd
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Convolution1D
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers

class Attention(tf.keras.Model):
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, features, hidden):
        hidden_with_time_axis = tf.expand_dims(hidden, 1)
        score = tf.nn.tanh(
            self.W1(features) + self.W2(hidden_with_time_axis))

        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

"""#### Embedding Layer"""

sequence_input = Input(shape=(MAX_LEN,), dtype="int32")
embedded_sequences = Embedding(MAX_FEATURES, EMBED_SIZE)(sequence_input)

"""#### Bi-Directional RNN"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, Concatenate, Embedding
from tensorflow.keras.models import Model

# Define input and embedding layer
sequence_input = Input(shape=(None,), dtype='int32')
embedding_layer = Embedding(input_dim=10000, output_dim=128)  # example dimensions
embedded_sequences = embedding_layer(sequence_input)

# Define LSTM layers
RNN_CELL_SIZE = 32  # Example size
lstm = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences=True), name="bi_lstm_0")(embedded_sequences)
(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences=True, return_state=True), name="bi_lstm_1")(lstm)

# Concatenate forward and backward states
state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

# Attention layer
context_vector, attention_weights = Attention(10)(lstm, state_h)

# Define fully connected layers
dense1 = Dense(20, activation="relu")(context_vector)
dropout = Dropout(0.05)(dense1)
output = Dense(1, activation="sigmoid")(dropout)

# Create and compile model
model = Model(inputs=sequence_input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

keras.utils.plot_model(model, show_shapes=True, dpi=90)

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
]
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)

BATCH_SIZE = 100
EPOCHS = 5
history = model.fit(X_train,y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_split=0.2)

y_test=df_test['sentiment']

tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(df_test['cleaned_text'])
list_tokenized_train = tokenizer.texts_to_sequences(df_test['cleaned_text'])

X_test = pad_sequences(list_tokenized_train, maxlen=MAX_LEN)

prediction = model.predict(X_test)

y_pred = (prediction > 0.5)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (classification_report,
                             confusion_matrix,
                             roc_auc_score)
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

report = classification_report(y_test, y_pred)
print(report)


def plot_cm(labels, predictions, p=0.5):
    cm = confusion_matrix(labels, predictions)
    plt.figure(figsize=(5, 5))
    sns.heatmap(cm, annot=True, fmt="d")
    plt.title("Confusion matrix (non-normalized))")
    plt.ylabel("Actual label")
    plt.xlabel("Predicted label")


plot_cm(y_test, y_pred)

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.TruePositives(name='tp'),
        tf.keras.metrics.FalsePositives(name='fp'),
        tf.keras.metrics.TrueNegatives(name='tn'),
        tf.keras.metrics.FalseNegatives(name='fn'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall'),
        tf.keras.metrics.AUC(name='auc')
    ]
)

plot_metrics(history)

"""## Fine-tuned with BERT"""

pip install datasets

import torch
from transformers import AutoTokenizer
from datasets import Dataset

## check if cuda is avaliable
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("training with:", device)

# setting up feature and target
yelp_dataset['sentiment'] = yelp_dataset['stars_x'].apply(lambda x: 1 if x > 3 else 0)  # Assuming 'stars_x' is the rating column

X = yelp_dataset['text'].values  # Assuming 'text' is the column with review content
Y = yelp_dataset['sentiment'].values

# train test split
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

## this is a binary classification problem, we will use accuracy
## let's check the default accuray by gussing all reviews are positive

default_accuracy = Y.mean()
print("The model is only meaningful when accuracy is above ",default_accuracy)

## Let's also initialize a dictionary to save the performance metric
performance = {"Default Accuracy": Y.mean()}
print(performance)

# preprare dataset
train_dataset = [dict(text=text, label=label) for text, label in zip(X_train, Y_train)]
test_dataset = [dict(text=text, label=label) for text, label in zip(X_test, Y_test)]

train_dataset = Dataset.from_list(train_dataset)
test_dataset = Dataset.from_list(test_dataset)

## this is how the dataset looks like
train_dataset[0]

## initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(data):
    return tokenizer(data['text'], padding="max_length", truncation=True)

## tokenize data
tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True)
tokenized_test_datasets = test_dataset.map(tokenize_function, batched=True)

## since fine tune takes a long time, I will randomly sample a subset for training
## in practice, you can use training sets  2000 < N < 100000, based on how difficult your problem is
tokenized_sample_train = tokenized_train_datasets.shuffle(seed=42).select(range(80000))  # randomly sample 80000 for training
tokenized_sample_test = tokenized_test_datasets.shuffle(seed=42).select(range(20000))  # randomly sample 20000 for validating during the training

!pip install transformers[torch] -U
!pip install accelerate -U
!pip install evaluate -U

import accelerate
print("Accelerate version:", accelerate.__version__)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np
import torch

# Initialize the model
model_checkpoint = "bert-base-cased"
num_labels = 2  # Adjust this depending on the number of labels in your classification task
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels).to(device)
metric = evaluate.load("accuracy")

# Use accuracy as evaluation metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Specify the training parameters
training_args = TrainingArguments(
    output_dir="test_trainer",
    num_train_epochs=2,  # Total 2 epochs
    evaluation_strategy="steps",
    eval_steps=100,  # Evaluate every 100 steps
    learning_rate=5e-5,  # It's good practice to specify learning rate
    per_device_train_batch_size=16,  # Batch size for training
    per_device_eval_batch_size=16   # Batch size for evaluation
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_sample_train,
    eval_dataset=tokenized_sample_test,
    compute_metrics=compute_metrics,
)

trainer.train()

"""### RoBERTa-sentiment"""

from transformers import pipeline

## initialize the model pipeline
RoBERTa_sentiment = pipeline(model="siebert/sentiment-roberta-large-english", truncation=True, device=device)

print(RoBERTa_sentiment("I love this!"))
print(RoBERTa_sentiment("I don't like this!"))

# Ensure X_test is a list of strings
X_test_text = [str(text) for text in restaurant_df['text'][:100]]

# Evaluate all test instances
# Truncation is automatically handled by the pipeline if not already set
y_pred = [RoBERTa_sentiment(text) for text in X_test_text]

# Assuming your function returns predictions as 'LABEL_1', 'LABEL_2', or 'LABEL_0'
# Map predicted labels to your numeric categories
y_pred_mapped = [1 if pred == 'LABEL_1' else (2 if pred == 'LABEL_2' else 0) for pred in y_pred]

# y_test is accessible and correctly prepared
accuracy = accuracy_score(y_test, y_pred_mapped)

# Initialize performance dictionary if not already
performance = {}
performance["RoBERTa-sentiment"] = accuracy

# Output the accuracy
print("RoBERTa sentiment analysis accuracy:", accuracy)

from transformers import pipeline
from sklearn.metrics import accuracy_score

# Load RoBERTa sentiment analysis model
RoBERTa_sentiment = pipeline("sentiment-analysis", model="roberta-base", truncation=True)

## Check the performance of RoBERTa-sentiment on 100 testing instances
y_pred = [RoBERTa_sentiment(text) for text in X_test[:100]]  # only evaluating the first 100 test instances to save time
y_pred = [pred[0]['label'] for pred in y_pred]
y_pred = [1 if pred == 'POSITIVE' else 0 for pred in y_pred]
accuracy = accuracy_score(Y_test[:100], y_pred)
performance["RoBERTa-sentiment"] = accuracy

"""#4. Sentiment Analysis ðŸŸ¡ Rami

**Bag of word and supervised learning**


1.   Logistic Regression
2.   Random Forest
3.   KNN
4.   Naive
5.   TIFF Model

* Use classification metric: F1-score to compare the modelâ€™s results.

Recurrent Neural Networks

- predict sentiment
"""

import seaborn as sns
sns.countplot(x='sentiment', data=restaurant_df)

!pip install keras

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array
import tensorflow as tf
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential
from keras.layers import Activation, Dropout, Dense
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split
X = []
sentences = list(restaurant_df['cleaned_text'])
for sen in sentences:
    X.append(sen)

y = restaurant_df['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(X_train)

X_train = word_tokenizer.texts_to_sequences(X_train)
X_test = word_tokenizer.texts_to_sequences(X_test)

import io
import json

tokenizer_json = word_tokenizer.to_json()
with io.open('b3_tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))
# Adding 1 to store dimensions for words for which no pretrained word embeddings exist

vocab_length = len(word_tokenizer.word_index) + 1

vocab_length

maxlen = 100

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)





"""**Transfer Learning**
- predict sentiment
"""

!pip install torch
!pip install torchvision
!pip install torchaudio

!pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

reviews = restaurant_df['text']

from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis")
data = restaurant_df['text'][4586]
sentiment_pipeline(data)

# Using a specific model for sentiment analysis
specific_model = pipeline(model="finiteautomata/bertweet-base-sentiment-analysis")
specific_model(restaurant_df['text'][24])

restaurant_df['text'][24]



"""Pretrained naive bayes"""

from textblob import TextBlob

restaurant_df['text'][25]

## test
blob = TextBlob(restaurant_df['text'][25])
if blob.sentiment.polarity >=0:
     print('positive')
else:
    print('negative')

"""Roberta sentiment

"""

from transformers import pipeline

## initialize the model pipeline
RoBERTa_sentiment = pipeline(model="siebert/sentiment-roberta-large-english", truncation=True)

## Check the performance of RoBERTa-sentiment on 100 testing instances
y_pred = RoBERTa_sentiment(restaurant_df['text'][0])  # only evaluating the first 100 test instances to save time
y_pred

"""LSTM

"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN
from sklearn.model_selection import train_test_split

# Assuming 'df' is your DataFrame with columns 'text' and 'sentiment'
X = restaurant_df['text'].values
y = restaurant_df['sentiment'].values

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
max_seq_length = max([len(seq) for seq in X_seq])
X_pad = pad_sequences(X_seq, maxlen=max_seq_length)

X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))
model.add(LSTM(32))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy}")

"""RNN

"""

vocab_size = len(tokenizer.word_index) + 1

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_seq_length))
model.add(SimpleRNN(units=64))  # Simple RNN layer
model.add(Dense(units=1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy}")

restaurant_df

from transformers import BertTokenizer
import torch

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize text samples and prepare inputs
def tokenize_text(text):
    return tokenizer.encode_plus(text,
                                 add_special_tokens=True,
                                 max_length=128,  # Max length of input text
                                 padding='max_length',
                                 return_attention_mask=True,
                                 truncation=True)

# Tokenize all text samples in the dataset
tokenized_texts = restaurant_df['text'][:100].apply(tokenize_text)

# Convert tokenized texts and labels into tensors
input_ids = torch.tensor([tokenized_texts[i]['input_ids'] for i in range(len(tokenized_texts))])
attention_masks = torch.tensor([tokenized_texts[i]['attention_mask'] for i in range(len(tokenized_texts))])
labels = torch.tensor(restaurant_df['sentiment'][:100].values)